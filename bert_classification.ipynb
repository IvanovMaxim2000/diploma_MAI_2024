{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgFgn95w4bdMosYbtLhUye"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zl48wc0VfY8_"
      },
      "outputs": [],
      "source": [
        "pip install transformers[torch]\n",
        "pip install accelerate -U\n",
        "pip install torch\n",
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from datasets import load_dataset, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
      ],
      "metadata": {
        "id": "Qtm4ZzJifpZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import torch\n",
        "import pickle\n",
        "import sklearn\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModel,AutoTokenizer,AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from nltk.corpus import stopwords\n",
        "from lightgbm import LGBMClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from keras.utils import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import AutoModel, BertModel, AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "7FDTksWHfsPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_dataset('csv', data_files={'train': 'df_file.csv'})\n",
        "\n",
        "# Преобразование меток в числовой формат (если необходимо)\n",
        "data = data['train'].map(lambda example: {'label': int(example['Label'])})\n",
        "\n",
        "# Выбор и загрузка модели\n",
        "model_name = \"bert-base-uncased\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Токенизация\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['Text'], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "data = data.map(tokenize_function, batched=True)\n",
        "\n",
        "# Разделение на обучающую и тестовую выборки\n",
        "train_data, test_data = train_test_split(data, test_size=0.2)\n",
        "\n",
        "train_data = Dataset.from_dict(train_data)\n",
        "test_data = Dataset.from_dict(test_data)\n",
        "\n",
        "# Обучение модели\n",
        "training_args = TrainingArguments(output_dir=\"./results\", evaluation_strategy=\"epoch\")\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=train_data, eval_dataset=test_data)\n",
        "\n",
        "# Проверка данных перед обучением\n",
        "print(train_data[0])\n",
        "\n",
        "# Запуск обучения\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "EPNzTCBVf3aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# получение предсказаний модели\n",
        "predictions = trainer.predict(test_dataset)\n",
        "predicted_labels = predictions.predictions.argmax(-1)\n",
        "true_labels = test_dataset['Label']\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1-score: {f1}')\n",
        "\n",
        "# 5. Сохранение модели\n",
        "trainer.save_model(\"./bert_res\")"
      ],
      "metadata": {
        "id": "GWndPjTDf8Fp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}